#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
DDPG-PC Training Script
Direct Python-CarSim DLL Link
"""
import numpy as np
import torch
import os
import random
from datetime import datetime
from torch.utils.tensorboard import SummaryWriter

# å‡è®¾æ–‡ä»¶ç»“æ„:
# ./train_ddpg_PC.py
# ./env_pc.py
# ./ddpg_agent.py
# ./pycarsimlib/ (åº“æ–‡ä»¶)
from ddpg_agent import DDPGAgent
from env_pc import PythonCarsimEnv  # å¼•ç”¨ä¸Šé¢æ–°å†™çš„ç±»

def setup_seed(seed):
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    random.seed(seed)
    torch.backends.cudnn.deterministic = True
    print(f"éšæœºç§å­å·²é”å®šä¸º: {seed}")

def train_ddpg_PC(
    max_episodes: int = 200,
    max_torque: float = 1500.0,
    target_slip_ratio: float = 0.1,
    target_speed: float = 100.0,
    log_dir: str = "logs_PC",
    pretrained_model_path: str = None 
):
    # --- 1. é…ç½® ---
    reward_weights = {
        'w_speed': 0.5,        # æé«˜ä¸€ç‚¹é€Ÿåº¦æƒé‡
        'w_accel': 0.0,
        'w_energy': 0.05,      # èƒ½è€—æƒ©ç½š
        'w_consistency': 0.0, 
        'w_beta': 0.0,       
        'w_slip': -0.03,        # å¼ºåŠ›æƒ©ç½šæ»‘ç§»
        'w_smooth': -0
    }
    
    hyperparams = {
        'Action Bound': 1.0,   
        'Hidden Dim': 256,
        'Actor LR': 1e-5,      
        'Critic LR': 1e-4,
        'Batch Size': 128,
        'Elite Ratio': 0.3,    
        'Elite Capacity': 20000,
        'Noise Scale': 0.5,    
        'Min Noise': 0.05,
        'Noise Decay': 0.998,  
    }
    
    # æ—¥å¿—
    current_time = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_path = os.path.join(log_dir, f"Python_Carsim_{current_time}")
    os.makedirs(log_path, exist_ok=True)
    writer = SummaryWriter(log_dir=log_path)
    print(f"è®­ç»ƒæ—¥å¿—: {log_path}")

    # --- 2. åˆå§‹åŒ–ç¯å¢ƒ ---
    # [å…³é”®ä¿®å¤] è¿™é‡Œå¿…é¡»æ˜¯ CarSim çš„ Database è·¯å¾„ (åŒ…å« Runs, Data, Extensions ç­‰æ–‡ä»¶å¤¹)
    # ä½ ä¹‹å‰å†™çš„ "Program Files" è·¯å¾„é€šå¸¸æ˜¯å®‰è£…è·¯å¾„ï¼Œä¸æ˜¯æ•°æ®è·¯å¾„ã€‚
    # è¯·æ£€æŸ¥ Public Documents æˆ–è€…ä½ è‡ªå·±çš„å·¥ä½œåŒº
    CARSIM_DB_DIR = r"E:\CarSim2022\CarSim2022.1_Prog\RL" 
    
    # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨
    if not os.path.exists(CARSIM_DB_DIR):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ° CarSim æ•°æ®åº“è·¯å¾„: {CARSIM_DB_DIR}")
        print("è¯·ä¿®æ”¹ä»£ç ä¸­çš„ CARSIM_DB_DIR ä¸ºåŒ…å« 'Runs' å’Œ 'Data' æ–‡ä»¶å¤¹çš„ç›®å½•")
        return

    env = PythonCarsimEnv(
        carsim_db_dir=CARSIM_DB_DIR,
        sim_time_s=10.0,       
        delta_time_s=0.01,
        max_torque=max_torque,
        target_slip_ratio=target_slip_ratio,
        target_speed=target_speed,
        vehicle_type="normal_vehicle", # ç¡®ä¿ä¸ pycarsimlib é‡Œçš„é…ç½®ä¸€è‡´
        reward_weights=reward_weights
    )
    
    # --- 3. åˆå§‹åŒ– Agent ---
    agent = DDPGAgent(
        state_dim=env.get_state_dim(),
        action_dim=env.get_action_dim(),
        action_bound=hyperparams['Action Bound'],
        hidden_dim=hyperparams['Hidden Dim'],
        actor_lr=hyperparams['Actor LR'],
        critic_lr=hyperparams['Critic LR'],
        batch_size=hyperparams['Batch Size'],
        elite_ratio=hyperparams['Elite Ratio'],
        elite_capacity=hyperparams['Elite Capacity']   
    )
    
    # åŠ è½½é¢„è®­ç»ƒ
    if pretrained_model_path and os.path.exists(pretrained_model_path):
        print(f"ğŸ”„ åŠ è½½é¢„è®­ç»ƒæ¨¡å‹: {pretrained_model_path}")
        agent.load_model(pretrained_model_path)
        noise_scale = 0.1 
    else:
        print("ğŸ†• ä»é›¶å¼€å§‹è®­ç»ƒ")
        noise_scale = hyperparams['Noise Scale']

    best_episode_reward = -float('inf') 
    min_noise = hyperparams['Min Noise']
    noise_decay = hyperparams['Noise Decay']

    print("\n========== Start Pure DDPG Training ==========")
    
    try:
        for episode in range(max_episodes):
            # 1. Reset (è¿™ä¸€æ­¥ä¼šé‡å¯ CarSim)
            state, info = env.reset()
            agent.reset_noise() 
            
            episode_reward = 0  
            reward_stats = { "R_Spd": [], "R_Slp": [], "R_Eng": [] }
            current_episode_memory = []
            
            critic_grads = []
            actor_grads = []
            
            while True:
                # 2. Select Action
                action = agent.select_action(state, noise_scale=noise_scale)

                # 3. Step
                next_state, reward, done, info = env.step(action)
                
                # 4. Push & Train
                agent.push(state, action, reward, next_state, done)
                current_episode_memory.append((state, action, reward, next_state, done))
                
                # DDPG
                c_loss, a_loss, c_grad, a_grad = agent.train_step()
                
                if c_loss != 0:
                    critic_grads.append(c_grad)
                    actor_grads.append(a_grad)
                
                state = next_state
                episode_reward += reward
                
                # Log Stats
                for k in reward_stats:
                    if k in info: reward_stats[k].append(info[k])
                
                if done: break
            
            # --- Episode End ---
            
            # Summary stats
            sum_rewards = {k: np.sum(v) for k, v in reward_stats.items()}
            avg_c = np.mean(critic_grads) if critic_grads else 0
            avg_a = np.mean(actor_grads) if actor_grads else 0

            # Tensorboard
            writer.add_scalar('Loss/Critic', c_loss, episode)
            writer.add_scalar('Loss/Actor', a_loss, episode)
            writer.add_scalar('Train/Reward', episode_reward, episode)
            writer.add_scalar('Train/Noise', noise_scale, episode)
            if avg_c > 0:
                writer.add_scalar('Grad/Critic', avg_c, episode)
                writer.add_scalar('Grad/Actor', avg_a, episode)

            # æ‰“å° Summary (è¦†ç›–æ‰ step çš„æ‰“å°)
            print(f"Ep {episode}| Rw: {episode_reward:.0f} | Ns: {noise_scale:.2f} | "
                  f"Spd: {sum_rewards['R_Spd']:.0f} | Slp: {sum_rewards['R_Slp']:.0f} | "
                  f"Grad: {avg_c:.3f}/{avg_a:.3f}")

            # ç²¾è‹±ç­–ç•¥
            is_elite = False
        if episode_reward > best_episode_reward*0.8 and episode_reward >=0:
            is_elite = True
            writer.add_scalar('Train/Is_Elite', 1, episode)
            print(f"ğŸŒŸ [ç²¾è‹±]! Reward: {episode_reward:.1f}")
            for trans in current_episode_memory:
                agent.push_elite(*trans)
            if episode_reward > best_episode_reward:
                best_episode_reward = episode_reward
                agent.save_model(os.path.join("best_model_save", f"Python_Carsim_{current_time}.pt"))
                print(f"ğŸŒŸ [æ–°çºªå½•] ! Reward: {episode_reward:.1f}")
        else:
            writer.add_scalar('Train/Is_Elite', 0, episode)
            
            # å™ªå£°è¡°å‡
            noise_scale = max(min_noise, noise_scale * noise_decay)

    except KeyboardInterrupt:
        print("äººä¸ºåœæ­¢è®­ç»ƒ...")
    except Exception as e:
        print(f"å‘ç”Ÿé”™è¯¯: {e}")
    finally:
        # ç¡®ä¿å…³é—­ CarSimï¼Œå¦åˆ™ä¸‹æ¬¡å¯èƒ½èµ·ä¸æ¥
        env.close()
        agent.save_model(os.path.join(log_path, "final_model.pt"))
        print("èµ„æºå·²é‡Šæ”¾ï¼Œè®­ç»ƒç»“æŸã€‚")

if __name__ == "__main__":
    setup_seed(42)
    # æ³¨æ„ï¼šä¸éœ€è¦ä¼ å…¥ pretrained_model_path=Noneï¼Œå› ä¸ºè¿™æ˜¯é»˜è®¤å€¼
    train_ddpg_PC()
